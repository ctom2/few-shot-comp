{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metadl_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmo2FsFgv7mh"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# make new directory with my implementation based\n",
        "!cp -r ../baselines/transfer ../baselines/submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPkHHmWPO7Jv"
      },
      "source": [
        "# ###############################################################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulkwVu85kUx"
      },
      "source": [
        "%%writefile ../baselines/submission/config.gin\n",
        "\n",
        "DataGenerator.batch_config = [28, 256] # [image_size, batch_size]\n",
        "DataGenerator.episode_config = [28, 5, 1, 5] # Not used \n",
        "DataGenerator.valid_episode_config = [28, 5, 1, 19]\n",
        "DataGenerator.pool = 'train'\n",
        "DataGenerator.mode = 'batch'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx5Toz0AOaes"
      },
      "source": [
        "%%writefile ../baselines/submission/model.py\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import csv \n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gin\n",
        "\n",
        "from metadl.api.api import MetaLearner, Learner, Predictor\n",
        "\n",
        "from tensorflow.keras.models import clone_model, Model\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dense, Dropout, Activation, MaxPooling2D\n",
        "from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D, Flatten, ZeroPadding2D, Add, AveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import ReLU, concatenate\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "LEARNING_RATE=0.00025\n",
        "DATASET_DIM=4096\n",
        "\n",
        "\n",
        "def basic_block(x, filters, stride=1):\n",
        "    x_skip = x\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    if stride != 1:\n",
        "      x_skip = Conv2D(filters, kernel_size=(1, 1), strides=stride)(x_skip)\n",
        "      x_skip = BatchNormalization()(x_skip)\n",
        "\n",
        "    x = Add()([x, x_skip])\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def make_block(x, filters, blocks, stride=1):\n",
        "    x = basic_block(x, filters, stride)\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "        x = basic_block(x, filters, stride=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def my_net(img_size=28):\n",
        "    inputs = Input(shape=(img_size, img_size, 3))\n",
        "    x = ZeroPadding2D(padding=(2, 2))(inputs)\n",
        "\n",
        "    x = Conv2D(64, kernel_size=(3, 3))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = make_block(x, 64, blocks=2)\n",
        "    x = make_block(x, 64, blocks=2, stride=1)\n",
        "    x = make_block(x, 64, blocks=2, stride=1)\n",
        "    x = make_block(x, 64, blocks=2, stride=1)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    outputs = Flatten()(x)\n",
        "\n",
        "    # define the model \n",
        "\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "\n",
        "@gin.configurable\n",
        "class MyMetaLearner(MetaLearner):\n",
        "    \"\"\" Loads and fine-tune a model pre-trained on ImageNet. \"\"\"\n",
        "    def __init__(self,\n",
        "                iterations=10,\n",
        "                freeze_base=True,\n",
        "                total_meta_train_class=883):\n",
        "        super().__init__()\n",
        "        self.iterations = iterations\n",
        "        # self.iterations = 200\n",
        "        self.epochs = 40\n",
        "        self.total_meta_train_class = total_meta_train_class\n",
        "\n",
        "        # ++++++ nets ++++++\n",
        "        inputs, my_net_outputs = my_net()\n",
        "        \n",
        "        x = BatchNormalization()(my_net_outputs)\n",
        "        outputs = Dense(self.total_meta_train_class, activation='softmax')(x)\n",
        "        self.model = Model(inputs, outputs)\n",
        "\n",
        "        logging.info(self.model.summary())\n",
        "\n",
        "        # x = BatchNormalization()(my_net_outputs)\n",
        "        # outputs = Dense(4, activation='softmax')(x)\n",
        "        # self.rotate_classifier = Model(inputs, outputs)\n",
        "        # ++++++ nets ++++++\n",
        "\n",
        "        self.loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "        # Summary Writers\n",
        "        self.current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.train_log_dir = ('logs/transfer/gradient_tape/' + self.current_time \n",
        "            + '/meta-train')\n",
        "        self.valid_log_dir = ('logs/transfer/gradient_tape/' + self.current_time \n",
        "            + '/meta-valid')\n",
        "        self.train_summary_writer = tf.summary.create_file_writer(\n",
        "            self.train_log_dir)\n",
        "        self.valid_summary_writer = tf.summary.create_file_writer(\n",
        "            self.valid_log_dir)\n",
        "\n",
        "        # Statstics tracker\n",
        "        self.train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
        "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "            name = 'train_accuracy')\n",
        "        self.valid_loss = tf.keras.metrics.Mean(name = 'valid_loss')\n",
        "        self.valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "            name = 'valid_accuracy')\n",
        "\n",
        "    def meta_fit(self, meta_dataset_generator) -> Learner:\n",
        "        \"\"\" We train the classfier created on top of the pre-trained embedding\n",
        "        layers.\n",
        "        Args:\n",
        "            meta_dataset_generator : a DataGenerator object. We can access \n",
        "                the meta-train and meta-validation episodes via its attributes.\n",
        "                Refer to the metadl/data/dataset.py for more details.\n",
        "        \n",
        "        Returns:\n",
        "            MyLearner object : a Learner that stores the current embedding \n",
        "                function (Neural Network) of this MetaLearner.\n",
        "        \"\"\"\n",
        "        meta_train_dataset = meta_dataset_generator.meta_train_pipeline\n",
        "        meta_valid_dataset = meta_dataset_generator.meta_valid_pipeline\n",
        "        meta_valid_dataset = meta_valid_dataset.batch(2)\n",
        "\n",
        "        logging.info('Starting meta-fit for the transfer baseline ...')\n",
        "        meta_iterator = meta_train_dataset.__iter__()\n",
        "        sample_data = next(meta_iterator)\n",
        "        logging.info('Images shape : {}'.format(sample_data[0][0].shape))\n",
        "        logging.info('Labels shape : {}'.format(sample_data[0][1].shape))\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            self.evaluate(MyLearner(self.model), meta_valid_dataset)\n",
        "            count = 0\n",
        "\n",
        "            # PT-MAP hyperparameter testing\n",
        "\n",
        "            # learning rate optimatization\n",
        "            if (epoch == 20) or (epoch == 30):\n",
        "                logging.info('New learning rate : {}'.format(epoch))\n",
        "                self.learning_rate = self.learning_rate/10\n",
        "                self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "\n",
        "            for (images, labels), _ in meta_train_dataset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    augmentation_type = np.random.randint(3)\n",
        "                    if augmentation_type == 0:\n",
        "                        images = tf.image.adjust_saturation(images, 3)\n",
        "                    elif augmentation_type == 1:\n",
        "                        images = tf.image.adjust_brightness(images, 0.4)\n",
        "\n",
        "                    images = tf.concat([images,\n",
        "                                        tf.image.rot90(images, k=1),\n",
        "                                        tf.image.rot90(images, k=2),\n",
        "                                        tf.image.rot90(images, k=3)], 0)\n",
        "                    \n",
        "                    labels = tf.concat([labels, labels, labels, labels], 0)\n",
        "\n",
        "                    cpreds = self.model(images)\n",
        "                    closs = self.loss(labels, cpreds)\n",
        "\n",
        "                grads = tape.gradient(closs, self.model.trainable_weights)\n",
        "                self.optimizer.apply_gradients(\n",
        "                    zip(grads, self.model.trainable_weights))\n",
        "\n",
        "                # grads = tape.gradient(rloss, self.rotate_classifier.trainable_weights)\n",
        "                # self.optimizer.apply_gradients(\n",
        "                #     zip(grads, self.rotate_classifier.trainable_weights))\n",
        "\n",
        "                if ((count + 1) % 10) == 0:\n",
        "                    logging.info('Epoch {}/{} - Iteration {}/{} - Loss : {}'.format(\n",
        "                        epoch + 1, self.epochs, count + 1, self.iterations, closs.numpy()))\n",
        "                # self.train_accuracy.update_state(labels, preds)\n",
        "                # self.train_loss.update_state(loss)\n",
        "\n",
        "                count += 1\n",
        "                if count >= self.iterations:\n",
        "                    break\n",
        "\n",
        "        return MyLearner(self.model)\n",
        "\n",
        "    def evaluate(self, learner, meta_valid_generator):\n",
        "        \"\"\"Evaluates the current meta-learner with episodes generated from the\n",
        "        meta-validation split. The number of episodes used to compute the \n",
        "        an average accuracy is set to 20.\n",
        "        Args:\n",
        "            learner : MyLearner object. The current state of the meta-learner \n",
        "                    is embedded in the object via its neural network.\n",
        "            meta_valid_generator : a tf.data.Dataset object that generates\n",
        "                                    episodes from the meta-validation split.\n",
        "        \"\"\"\n",
        "        count_val = 0\n",
        "        for tasks_batch in meta_valid_generator : \n",
        "            sup_set = tf.data.Dataset.from_tensor_slices(\n",
        "                (tasks_batch[0][1], tasks_batch[0][0]))\n",
        "            que_set = tf.data.Dataset.from_tensor_slices(\n",
        "                (tasks_batch[0][4],tasks_batch[0][3]))\n",
        "            new_ds = tf.data.Dataset.zip((sup_set, que_set))\n",
        "            for ((supp_labs, supp_img), (que_labs, que_img)) in new_ds:\n",
        "                # supp_img, que_img = self.aug_rotation(supp_img, que_img)\n",
        "                support_set = tf.data.Dataset.from_tensor_slices(\n",
        "                    (supp_img, supp_labs))\n",
        "                query_set = tf.data.Dataset.from_tensor_slices(que_img)\n",
        "                support_set = support_set.batch(5)\n",
        "                query_set = query_set.batch(95)\n",
        "                predictor = learner.fit(support_set)\n",
        "                preds = predictor.predict(query_set)\n",
        "                self.valid_accuracy.update_state(que_labs, preds)\n",
        "            \n",
        "            count_val += 1 \n",
        "            if count_val >= 20: break\n",
        "\n",
        "        logging.info('Meta-Valid accuracy : {:.3%}'.format(\n",
        "            self.valid_accuracy.result()))\n",
        "\n",
        "\n",
        "@gin.configurable\n",
        "class MyLearner(Learner):\n",
        "    def __init__(self, \n",
        "                model=None,\n",
        "                N_ways=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model : A keras.Model object describing the Meta-Learner's neural\n",
        "                network.\n",
        "            N_ways : Integer, the number of classes to consider at meta-test\n",
        "                time.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.N_ways = N_ways\n",
        "        self.K_shots = 1\n",
        "\n",
        "        if model == None:\n",
        "            inputs, outputs = my_net()\n",
        "            # outputs = Dense(self.total_meta_train_class)(outputs)\n",
        "            self.model = Model(inputs, outputs)\n",
        "\n",
        "        else : \n",
        "            new_model = keras.models.clone_model(model)\n",
        "            self.model = keras.Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "            # self.model = new_model\n",
        "\n",
        "        # logging.info(self.model.summary())\n",
        "\n",
        "        # self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        # self.loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "        self.dataset = torch.zeros((1, self.N_ways, self.K_shots, DATASET_DIM))\n",
        "\n",
        "    def fit(self, dataset_train) -> Predictor:\n",
        "        \"\"\"Fine-tunes the current model with the support examples of a new \n",
        "        unseen task. \n",
        "        Args:\n",
        "            dataset_train : a tf.data.Dataset object. Iterates over the support\n",
        "                examples. \n",
        "        Returns:\n",
        "            a Predictor object that is initialized with the fine-tuned \n",
        "                Learner's neural network weights.\n",
        "        \"\"\"\n",
        "        for sup_imgs, sup_lbls in dataset_train:\n",
        "            emb_imgs = self.model(sup_imgs)\n",
        "            sup_lbls_numpy = sup_lbls.numpy()\n",
        "\n",
        "            for emb_img, sup_lbl in zip(emb_imgs, sup_lbls_numpy):\n",
        "                emb_img_torch = torch.from_numpy(emb_img.numpy())\n",
        "                self.dataset[0, sup_lbl, 0, :] = emb_img_torch\n",
        "\n",
        "        return MyPredictor(self.model, self.dataset)\n",
        "\n",
        "    def save(self, model_dir):\n",
        "        \"\"\"\n",
        "        Saves the embedding function, i.e. the prototypical network as a \n",
        "        tensorflow checkpoint.\n",
        "        \"\"\"\n",
        "        if(os.path.isdir(model_dir) != True):\n",
        "            raise ValueError('The model directory provided is invalid. Please\\\n",
        "                 check that its path is valid.')\n",
        "        \n",
        "        ckpt_file = os.path.join(model_dir, 'learner.ckpt')\n",
        "        self.model.save_weights(ckpt_file) \n",
        "\n",
        "    def load(self, model_dir):\n",
        "        \"\"\"\n",
        "        Loads the embedding function, i.e. the prototypical network from a \n",
        "        tensorflow checkpoint.\n",
        "        \"\"\"\n",
        "        if(os.path.isdir(model_dir) != True):\n",
        "            raise ValueError('The model directory provided is invalid. Please\\\n",
        "                    check that its path is valid.')\n",
        "\n",
        "        ckpt_path = os.path.join(model_dir, 'learner.ckpt')\n",
        "        self.model.load_weights(ckpt_path)\n",
        "\n",
        "    \n",
        "class MyPredictor(Predictor):\n",
        "    def __init__(self,\n",
        "                model,\n",
        "                dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_fn : Distance funtion to consider at meta-test time.\n",
        "            dataset : Prototypes computed using the support set\n",
        "            distance_fn : Distance function to consider for the proto-networks\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding_fn = model\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.config = {\n",
        "            'n_shot' : 1,\n",
        "            'n_ways' : 5,\n",
        "            'n_queries' : 19,\n",
        "\n",
        "            'n_runs' : 1, # competition works only with one run for evaluations\n",
        "\n",
        "            'pt_epsilon' : 1e-6,\n",
        "            'pt_beta' : 0.5,\n",
        "            'gm_lambda' : 10,\n",
        "            'map_alpha' : 0.4,\n",
        "            'map_epochs': 30,\n",
        "        }\n",
        "\n",
        "        self.config['n_lsamples'] = self.config['n_ways'] * self.config['n_shot']\n",
        "        self.config['n_usamples'] = self.config['n_ways'] * self.config['n_queries']\n",
        "        self.config['n_samples'] = self.config['n_lsamples'] + self.config['n_usamples']\n",
        "\n",
        "    # centres support data and query data\n",
        "    def center_data(self, data, index):\n",
        "        # centering support data\n",
        "        data[:, :index] = data[:, :index, :] - data[:, :index].mean(1, keepdim=True)\n",
        "        data[:, :index] = data[:, :index, :] / torch.norm(data[:, :index, :], 2, 2)[:, :, None]\n",
        "        # centering query data\n",
        "        data[:, index:] = data[:, index:, :] - data[:, index:].mean(1, keepdim=True)\n",
        "        data[:, index:] = data[:, index:, :] / torch.norm(data[:, index:, :], 2, 2)[:, :, None]\n",
        "        \n",
        "        return data\n",
        "\n",
        "    # calculates the mu_j estimation\n",
        "    def mu_j_estimation(self, M_star, f_data):\n",
        "        # nominator\n",
        "        nominator = M_star.permute(0,2,1).matmul(f_data)\n",
        "        # denominator\n",
        "        denominator = M_star.sum(dim=1).unsqueeze(2)\n",
        "\n",
        "        return nominator.div(denominator)\n",
        "\n",
        "    # creates pytorch tensor with query data required for PT+MAP\n",
        "    def dataset_test_to_pytorch(self, dataset_test):\n",
        "        query_data = torch.zeros((\n",
        "            self.config['n_runs'],\n",
        "            self.config['n_ways'],\n",
        "            self.config['n_queries'],\n",
        "            self.dataset.shape[3]))\n",
        "\n",
        "        # only one iteration (the for loop is actually useless)\n",
        "        for que_imgs in dataset_test:\n",
        "            emb_imgs = self.embedding_fn(que_imgs)\n",
        "\n",
        "            i = 0\n",
        "            sup_lbl = 0\n",
        "\n",
        "            for emb_img in emb_imgs:\n",
        "                query_data[0, sup_lbl, i, :] = torch.from_numpy(emb_img.numpy())\n",
        "\n",
        "                if i == self.config['n_queries'] - 1: \n",
        "                    sup_lbl += 1\n",
        "                    i = 0\n",
        "                else: \n",
        "                    i += 1\n",
        "\n",
        "        return query_data\n",
        "\n",
        "    # necessary transformations for PT+MAP, returns transformed data and labels\n",
        "    def pt_map_init(self, ndatas):\n",
        "        ndatas = ndatas.permute(0,2,1,3)\n",
        "        ndatas = ndatas.reshape(\n",
        "            self.config['n_runs'], self.config['n_samples'], -1)\n",
        "            \n",
        "        # Power transform\n",
        "        ndatas[:,] = torch.pow(ndatas[:,] + self.config['pt_epsilon'], self.config['pt_beta'])\n",
        "\n",
        "        # QR reduction\n",
        "        ndatas = torch.qr(ndatas.permute(0,2,1)).R.permute(0,2,1)\n",
        "\n",
        "        self.config['n_nfeat'] = ndatas.size(2)\n",
        "        \n",
        "        # unit variance projection\n",
        "        ndatas = ndatas/ndatas.norm(dim=2, keepdim=True)\n",
        "\n",
        "        # trans-mean-sub\n",
        "        ndatas = self.center_data(ndatas, self.config['n_lsamples'])\n",
        "\n",
        "        # labels transformations\n",
        "        labels = torch.arange(self.config['n_ways']).view(1, 1, self.config['n_ways'])\n",
        "        labels = labels.expand(\n",
        "            self.config['n_runs'], self.config['n_shot'] + self.config['n_queries'], self.config['n_ways'])\n",
        "        labels = labels.clone().view(self.config['n_runs'], self.config['n_samples'])\n",
        "\n",
        "        return ndatas, labels\n",
        "\n",
        "    # converts pytorch probabilites to tensorflow + formatting to match the label order\n",
        "    def probs_to_tf(self, probs, labels):\n",
        "        # torch data into numpy arrays\n",
        "        numpy_probs = probs.cpu().numpy()\n",
        "        numpy_labels = labels.squeeze().cpu().numpy()\n",
        "\n",
        "        # concatenating the results (probabilities)\n",
        "        concat_probs = np.concatenate(\n",
        "            [numpy_probs[numpy_labels[self.config['n_lsamples']:] == index] for index in range(self.config['n_ways'])])\n",
        "\n",
        "        # converting the probabilities to tensorflow tensor for consistency\n",
        "        probs = tf.convert_to_tensor(concat_probs)\n",
        "\n",
        "        return probs\n",
        "\n",
        "    # returns first c_j estimations -> support classes means\n",
        "    def c_j_data_init(self, ndatas):\n",
        "        reshape_0 = self.config['n_runs']\n",
        "        reshape_1 = self.config['n_shot'] + self.config['n_queries']\n",
        "        reshape_2 = self.config['n_ways']\n",
        "        reshape_3 = self.config['n_nfeat']\n",
        "\n",
        "        tmp_data = ndatas.reshape(reshape_0, reshape_1, reshape_2, reshape_3)\n",
        "        tmp_data = tmp_data[:,:self.config['n_shot'],]\n",
        "\n",
        "        c_j_data = tmp_data.mean(1)\n",
        "\n",
        "        return c_j_data\n",
        "\n",
        "    '''\n",
        "    Estimate the optimal transport from the initial distribution of the feature vectors to one that \n",
        "    would correspond to a balanced draw of samples from Gaussian distributions.\n",
        "    '''\n",
        "    def compute_optimal_transport(self, M, p, q, epsilon=1e-6):\n",
        "        n_runs, n, m = M.shape\n",
        "        P = torch.exp(- self.config['gm_lambda'] * M)\n",
        "        P /= P.view((n_runs, -1)).sum(1).unsqueeze(1).unsqueeze(1)\n",
        "                                        \n",
        "        u = torch.zeros(n_runs, n)#.cuda()\n",
        "        maxiters = 1000\n",
        "        iters = 1\n",
        "        # normalize this matrix\n",
        "        while torch.max(torch.abs(u - P.sum(2))) > epsilon:\n",
        "            u = P.sum(2)\n",
        "            # rows features\n",
        "            P *= (p / u).view((n_runs, -1, 1))\n",
        "            # columns features\n",
        "            P *= (q / P.sum(1)).view((n_runs, 1, -1))\n",
        "            \n",
        "            if iters == maxiters: break\n",
        "            iters += 1\n",
        "        \n",
        "        return P\n",
        "\n",
        "    def compute_probabilities(self, dist, labels):   \n",
        "        # L = Sinkhorn -> L\n",
        "        L = dist[:, self.config['n_lsamples']:]\n",
        "        # p = Sinkhorn -> p\n",
        "        _1wq = torch.ones(self.config['n_runs'], self.config['n_usamples'])#.cuda()\n",
        "        # q = Sinkhorn -> q\n",
        "        q1w = (torch.ones(self.config['n_runs'], self.config['n_ways']) * self.config['n_queries'])#.cuda()\n",
        "        \n",
        "        p_xj_test = self.compute_optimal_transport(L, _1wq, q1w)\n",
        "\n",
        "        p_xj = torch.zeros_like(dist)\n",
        "        p_xj[:, self.config['n_lsamples']:] = p_xj_test\n",
        "        \n",
        "        p_xj[:,:self.config['n_lsamples']].fill_(0)\n",
        "        p_xj[:,:self.config['n_lsamples']].scatter_(2, labels[:,:self.config['n_lsamples']].unsqueeze(2), 1)\n",
        "        \n",
        "        return p_xj\n",
        "\n",
        "\n",
        "    def predict(self, dataset_test):\n",
        "        '''\n",
        "        Leveraging the Feature Distribution in Transfer-based Few-Shot Learning\n",
        "        Code: https://github.com/yhu01/PT-MAP/tree/b58829b6e24e0441d46b8e1d9ff3149553b4456d\n",
        "        Paper: https://arxiv.org/pdf/2006.03806.pdf\n",
        "\n",
        "        Algorithm authors:\n",
        "            Yuqing Hu, Electronics Dept.,IMT Atlantique, France - Orange Labs, France Cesson-Sévigné; email: yuqing.hu@imt-atlantique.fr\n",
        "            Vincent Gripon, Electronics Dept.,IMT Atlantique, France Brest; email: vincent.gripon@imt-atlantique.fr\n",
        "            Stéphane Pateux, Orange Labs, France Cesson-Sévigné; email: stephane.pateux@orange.com\n",
        "\n",
        "        The following implementation replicates the PT-MAP algorithm proposed by the authors mentioned above.\n",
        "        It also borrows inspiration from their pytorch implementation from GitHub.\n",
        "        '''\n",
        "\n",
        "        query_data = self.dataset_test_to_pytorch(dataset_test)\n",
        "\n",
        "        # putting the support data and the query data together\n",
        "        ndatas = torch.cat((self.dataset, query_data), 2)\n",
        "\n",
        "        ndatas, labels = self.pt_map_init(ndatas)\n",
        "\n",
        "        del dataset_test\n",
        "        # switch to cuda\n",
        "        # ndatas = ndatas.cuda()\n",
        "        # labels = labels.cuda()\n",
        "\n",
        "\n",
        "        # PT-MAP Algorithm 1 (page 5)\n",
        "\n",
        "        # first c_j data set\n",
        "        c_j_data = self.c_j_data_init(ndatas)\n",
        "\n",
        "        for epoch in range(0, self.config['map_epochs']):\n",
        "            # L_ij matrix creation\n",
        "            dist = (ndatas.unsqueeze(2) - c_j_data.unsqueeze(1)).norm(dim=3).pow(2)\n",
        "\n",
        "            # probas maker, Sinkhorn mapping\n",
        "            probas = self.compute_probabilities(dist, labels)\n",
        "\n",
        "            # mu_j estimation -> mu_j = g(M*,j)\n",
        "            mu_j_data = self.mu_j_estimation(probas, ndatas)\n",
        "                        \n",
        "            # update centroids\n",
        "            c_j_data = c_j_data + self.config['map_alpha'] * (mu_j_data - c_j_data)\n",
        "        \n",
        "        dist = (ndatas.unsqueeze(2) - c_j_data.unsqueeze(1)).norm(dim=3).pow(2)\n",
        "        torch_probs = self.compute_probabilities(dist, labels).squeeze()[self.config['n_lsamples']:]\n",
        "\n",
        "        # converting the probabilities to tensorflow tensor for consistency\n",
        "        probs = self.probs_to_tf(torch_probs, labels)\n",
        "\n",
        "        return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2zcBvtPCkQ"
      },
      "source": [
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iu1O1QtE41u"
      },
      "source": [
        "!python -m metadl.core.run --meta_dataset_dir=../../omniglot --code_dir=../baselines/submission"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}